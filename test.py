import json
import os
import logging
from difflib import SequenceMatcher

# Setup logging
logging.basicConfig(filename='qa_pipeline.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', encoding='utf-8')

def exact_question_check(query, json_files):
    """
    So s√°nh query v·ªõi c√°c c√¢u h·ªèi trong JSON. N·∫øu match, tr·∫£ c√¢u tr·∫£ l·ªùi.
    Input: query (str), json_files (list of file paths)
    Output: answer (str) n·∫øu match, None n·∫øu kh√¥ng
    """
    query = query.lower().strip()  # Chu·∫©n h√≥a query
    logging.info(f"Checking exact match for query: {query}")
    
    best_match = None
    best_similarity = 0
    
    # T·∫°o keywords t·ª´ query ƒë·ªÉ matching t·ªët h∆°n
    query_keywords = set(preprocess_query(query).split())
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for item in data['data']:
                    for paragraph in item['paragraphs']:
                        for qa in paragraph['qas']:
                            question = qa['question'].lower().strip()
                            
                            # Ki·ªÉm tra exact match tr∆∞·ªõc
                            if query == question:
                                answer = qa['answers'][0]['text']
                                logging.info(f"Exact match found in {json_file}, question: {question}, answer: {answer}")
                                return answer
                            
                            # Ki·ªÉm tra keyword overlap
                            question_keywords = set(preprocess_query(question).split())
                            keyword_overlap = len(query_keywords.intersection(question_keywords))
                            
                            # Ki·ªÉm tra similarity cao
                            similarity = SequenceMatcher(None, query, question).ratio()
                            
                            # T√≠nh combined score
                            combined_score = similarity + 0.1 * keyword_overlap
                            
                            if combined_score > best_similarity and combined_score > 0.7:  # Gi·∫£m ng∆∞·ª°ng v√† th√™m keyword boost
                                best_similarity = combined_score
                                best_match = {
                                    'answer': qa['answers'][0]['text'],
                                    'question': question,
                                    'file': json_file,
                                    'similarity': combined_score
                                }
        except Exception as e:
            logging.error(f"Error reading {json_file}: {e}")
    
    # Tr·∫£ v·ªÅ best match n·∫øu c√≥
    if best_match and best_similarity > 0.7:
        logging.info(f"Best match found: {best_match['question']} (similarity: {best_similarity:.3f})")
        return best_match['answer']
    
    logging.info("No exact match found, proceeding to next steps")
    return None 
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from underthesea import word_tokenize

def preprocess_text(text):
    """Chu·∫©n h√≥a vƒÉn b·∫£n: lowercase, tokenize b·∫±ng Underthesea, b·ªè d·∫•u c√¢u"""
    text = text.lower().strip()
    tokens = word_tokenize(text, format='text')  # Underthesea tokenize
    return tokens

def load_data(json_files, cache_path='vectors.pkl'):
    """
    Load JSON files, index questions, and cache TF-IDF vectors
    Input: json_files (list of file paths), cache_path (str)
    Output: data (list of dicts), vectorizer, doc_vectors
    """
    data = []
    questions = []
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
                for item in json_data['data']:
                    for paragraph in item['paragraphs']:
                        for qa in paragraph['qas']:
                            data.append({
                                'file': json_file,
                                'question': qa['question'],
                                'answer': qa['answers'][0]['text'],
                                'context': paragraph['context']
                            })
                            questions.append(preprocess_text(qa['question']))
            logging.info(f"Loaded {json_file} with {len(json_data['data'])} FAQs")
        except Exception as e:
            logging.error(f"Error loading {json_file}: {e}")
            
    
    # Vectorize questions
    try:
        vectorizer = TfidfVectorizer()
        doc_vectors = vectorizer.fit_transform(questions)
        with open(cache_path, 'wb') as f:
            pickle.dump((vectorizer, doc_vectors), f)
        logging.info(f"Cached TF-IDF vectors to {cache_path}")
    except Exception as e:
        logging.error(f"Error vectorizing or caching: {e}")
        return data, None, None
    
    return data, vectorizer, doc_vectors
def load_stopwords():
    """T·∫£i danh s√°ch stop words ti·∫øng Vi·ªát (c√≥ th·ªÉ d√πng file ho·∫∑c danh s√°ch m·∫∑c ƒë·ªãnh)"""
    # Danh s√°ch stopwords t·ªëi thi·ªÉu ƒë·ªÉ gi·ªØ l·∫°i c√°c t·ª´ quan tr·ªçng
    stopwords = {'l√†', 'c·ªßa', 'v√†', 'trong', 'cho', 'ƒë∆∞·ª£c', 'c√≥', 'm·ªôt', 'c√°c', 'nh·ªØng', 'n√†y', 'ƒë√≥', 'n√†o', 'g√¨', 'khi', 'n·∫øu', 'ƒë·ªÉ', 'v·ªõi', 't·ª´', 'ƒë·∫øn', 't·∫°i', 'v·ªÅ', 'theo', 'nh∆∞', 'ho·∫∑c', 'c≈©ng', 'ƒë√£', 's·∫Ω', 'ƒëang', 'ƒë∆∞·ª£c', 'c√≥ th·ªÉ', 'ph·∫£i', 'c·∫ßn', 'n√™n', 'th√¨', 'm√†', 'ƒë·ªÉ', 'cho', 'v·ªÅ', 'c·ªßa', 'trong', 'v·ªõi', 't·ª´', 'ƒë·∫øn', 't·∫°i', 'theo', 'nh∆∞', 'ho·∫∑c', 'c≈©ng', 'ƒë√£', 's·∫Ω', 'ƒëang', 'ƒë∆∞·ª£c', 'c√≥ th·ªÉ', 'ph·∫£i', 'c·∫ßn', 'n√™n', 'th√¨', 'm√†'}
    return stopwords

def preprocess_query(query):
    """
    Preprocess query: lowercase, tokenize, remove punctuation, stop words
    Input: query (str)
    Output: processed query (str)
    """
    stopwords = load_stopwords()
    
    # Lowercase and tokenize
    query = query.lower().strip()
    tokens = word_tokenize(query, format='text')
    
    # Remove punctuation and stop words, but keep important words
    tokens = [word for word in tokens.split() if word not in stopwords and len(word) > 1]
    processed_query = ' '.join(tokens)
    
    logging.info(f"Preprocessed query: {processed_query}")
    return processed_query

from underthesea import ner

def map_ner_to_custom(entity, entity_type):
    """Map nh√£n Underthesea sang th·ª±c th·ªÉ t√πy ch·ªânh"""
    # X·ª≠ l√Ω encoding issues
    entity = entity.strip()
    if not entity or len(entity) < 2:
        return None
    
    
    if entity_type.startswith('B-') or entity_type.startswith('I-'):
        if 'PER' in entity_type:  # Person
            return 'Person'
        elif 'LOC' in entity_type:  # Location
            return 'Location'
        elif 'ORG' in entity_type:  # Organization
            return 'Organization'
        elif 'MISC' in entity_type:  # Miscellaneous
            return 'Misc'
    
    # Heuristic mapping d·ª±a tr√™n n·ªôi dung
    entity_lower = entity.lower()
    if entity.isnumeric() or 'tri·ªáu' in entity_lower or 'ngh√¨n' in entity_lower:
        return 'Amount'
    elif any(word in entity_lower for word in ['b·∫£o h√†nh', 'd·ªãch v·ª•', 'h·ªó tr·ª£']):
        return 'Service'
    elif any(word in entity_lower for word in ['h√†ng', 's·∫£n ph·∫©m', 'm√°y', 'ƒëi·ªán tho·∫°i', 'laptop']):
        return 'Product'
    elif any(word in entity_lower for word in ['gi√° tr·ªã', 'cao', 'quan tr·ªçng']):
        return 'Value'
    
    return None

def extract_keywords_from_query(query):
    """
    Tr√≠ch xu·∫•t keywords quan tr·ªçng t·ª´ query
    Input: query (str)
    Output: list of (keyword, type)
    """
    keywords = []
    query_lower = query.lower()
    
    # C√°c t·ª´ kh√≥a quan tr·ªçng
    important_phrases = [
        ('h√†ng gi√° tr·ªã cao', 'Product'),
        ('ƒë√≥ng g√≥i', 'Action'),
        ('v·∫≠n chuy·ªÉn', 'Action'),
        ('b·∫£o hi·ªÉm', 'Service'),
        ('th·ªùi gian', 'Time'),
        ('chi ph√≠', 'Cost'),
        ('gi·ªù l√†m vi·ªác', 'Time'),
        ('ƒë·ªìng ph·ª•c', 'Policy'),
        ('ngh·ªâ ph√©p', 'Policy'),
        ('l∆∞∆°ng', 'Policy')
    ]
    
    for phrase, phrase_type in important_phrases:
        if phrase in query_lower:
            keywords.append((phrase, phrase_type))
    
    return keywords

def ner_extraction(query):
    """
    Tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ query
    Input: query (str)
    Output: list of (entity, custom_type)
    """
    # Th·ª≠ NER tr∆∞·ªõc
    try:
        entities = ner(query)  # Underthesea NER
        custom_entities = []
        
        for entity, entity_type, _, _ in entities:
            mapped_type = map_ner_to_custom(entity, entity_type)
            if mapped_type:
                custom_entities.append((entity, mapped_type))
    except Exception as e:
        logging.warning(f"NER failed: {e}")
        custom_entities = []
    
    # Th√™m keyword extraction
    keyword_entities = extract_keywords_from_query(query)
    custom_entities.extend(keyword_entities)
    
    logging.info(f"NER entities: {custom_entities}")
    return custom_entities

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def semantic_search(query, data, vectorizer, doc_vectors, entities, top_k=5):
    """
    T√¨m FAQ t∆∞∆°ng t·ª± nh·∫•t b·∫±ng TF-IDF v√† cosine similarity, boost entity
    Input: query (str), data (list), vectorizer, doc_vectors, entities (list), top_k (int)
    Output: list of (index, score)
    """
    processed_query = preprocess_query(query)
    query_vec = vectorizer.transform([processed_query])
    
    # Cosine similarity
    similarities = cosine_similarity(query_vec, doc_vectors)[0]
    
    # Entity boosting
    for entity, _ in entities:
        if entity in vectorizer.vocabulary_:
            idx = vectorizer.vocabulary_[entity]
            similarities += 0.1 * (doc_vectors[:, idx].toarray().flatten())  # Boost entity
    
    # Get top-k indices
    top_indices = np.argsort(similarities)[::-1][:top_k]
    results = [(idx, similarities[idx]) for idx in top_indices]
    
    logging.info(f"Semantic search results: {results}")
    return results

def rule_based_matching(query, data, top_results, entities, threshold=0.3):
    """
    L·ªçc FAQ d·ª±a tr√™n entity v√† keyword, ki·ªÉm tra ng∆∞·ª°ng
    Input: query (str), data (list), top_results (list of (idx, score)), entities (list), threshold (float)
    Output: best index (int) ho·∫∑c None n·∫øu fallback
    """
    keywords = preprocess_query(query).split()
    best_idx, best_score = None, 0
    query_lower = query.lower()
    
    for idx, score in top_results:
        if score < threshold:
            continue
        question = data[idx]['question'].lower()
        context = data[idx]['context'].lower()
        
        # Entity matching v·ªõi tr·ªçng s·ªë cao
        entity_score = 0
        for entity, entity_type in entities:
            entity_lower = entity.lower()
            if entity_lower in question or entity_lower in context:
                # Boost score d·ª±a tr√™n lo·∫°i entity
                if entity_type in ['Product', 'Action', 'Service']:
                    entity_score += 0.3
                else:
                    entity_score += 0.1
        
        # Keyword matching
        keyword_count = sum(1 for kw in keywords if kw in question or kw in context)
        keyword_score = 0.1 * keyword_count
        
        # Exact phrase matching (quan tr·ªçng nh·∫•t)
        phrase_score = 0
        if len(keywords) >= 2:
            phrase = ' '.join(keywords)
            if phrase in question or phrase in context:
                phrase_score = 0.5
        
        # Special phrase matching cho c√°c c·ª•m t·ª´ quan tr·ªçng
        special_phrases = ['h√†ng gi√° tr·ªã cao', 'ƒë√≥ng g√≥i', 'gi·ªù l√†m vi·ªác', 'ƒë·ªìng ph·ª•c']
        special_score = 0
        for phrase in special_phrases:
            if phrase in query_lower and phrase in (question + ' ' + context):
                special_score += 0.4
        
        # Combined score
        combined_score = score + entity_score + keyword_score + phrase_score + special_score
        
        if combined_score > best_score:
            best_idx, best_score = idx, combined_score
            logging.info(f"New best match: idx={idx}, score={combined_score:.3f}, entity_score={entity_score:.3f}, keyword_score={keyword_score:.3f}, phrase_score={phrase_score:.3f}, special_score={special_score:.3f}")
    
    if best_idx is None:
        logging.warning("No match found, falling back")
        return None
    
    logging.info(f"Rule-based match: index {best_idx}, score {best_score}")
    return best_idx

def rerank_fusion(top_results, rule_idx, data):
    """
    Ch·ªçn FAQ t·ªët nh·∫•t t·ª´ semantic search v√† rule-based
    Input: top_results (list of (idx, score)), rule_idx (int or None), data (list)
    Output: best index (int) ho·∫∑c None
    """
    if rule_idx is not None:
        logging.info(f"Selected index {rule_idx} from rule-based matching")
        return rule_idx
    
    # N·∫øu rule-based kh√¥ng t√¨m th·∫•y, l·∫•y top-1 t·ª´ semantic search
    if top_results:
        best_idx = top_results[0][0]
        logging.info(f"Selected index {best_idx} from semantic search")
        return best_idx
    
    logging.warning("No match after reranking")
    return None

def postprocess_answer(answer, entities):
    """
    Ch√®n th·ª±c th·ªÉ v√†o c√¢u tr·∫£ l·ªùi v√† ƒë·ªãnh d·∫°ng
    Input: answer (str), entities (list of (entity, type))
    Output: formatted answer (str)
    """
    for entity, entity_type in entities:
        answer = answer.replace(f'[{entity_type}]', entity)
    
    logging.info(f"Formatted answer: {answer}")
    return answer

def detect_irrelevant_questions(query):
   
    query_lower = query.lower().strip()
    
    # C√°c m·∫´u c√¢u h·ªèi kh√¥ng li√™n quan
    irrelevant_patterns = [
        # Ch√†o h·ªèi c√° nh√¢n
        r'ch√†o\s+(b·∫°n|anh|ch·ªã|em|c√¥|ch√∫|b√°c)',
        r'xin\s+ch√†o',
        r'hello|hi|hey',
        r'ch√†o\s+bot',
        
        # H·ªèi s·ª©c kh·ªèe, c·∫£m x√∫c
        r'b·∫°n\s+(kh·ªèe|sao|th·∫ø\s+n√†o)',
        r'bot\s+(kh·ªèe|sao|th·∫ø\s+n√†o)',
        r'b·∫°n\s+c√≥\s+kh·ªèe\s+kh√¥ng',
        r'b·∫°n\s+th·∫ø\s+n√†o',
        
        # H·ªèi s·ªü th√≠ch, c√° nh√¢n
        r'b·∫°n\s+th√≠ch\s+g√¨',
        r'b·∫°n\s+c√≥\s+th√≠ch',
        r's·ªü\s+th√≠ch\s+c·ªßa\s+b·∫°n',
        r'b·∫°n\s+th√≠ch\s+ƒÉn\s+g√¨',
        r'b·∫°n\s+th√≠ch\s+ch∆°i\s+g√¨',
        
        # H·ªèi th·ªùi ti·∫øt, tin t·ª©c
        r'th·ªùi\s+ti·∫øt\s+h√¥m\s+nay',
        r'tin\s+t·ª©c',
        r'b√°o\s+ch√≠',
        r'tivi',
        
        # H·ªèi v·ªÅ AI, bot
        r'b·∫°n\s+l√†\s+ai',
        r'b·∫°n\s+l√†\s+g√¨',
        r'b·∫°n\s+c√≥\s+ph·∫£i\s+ai',
        r'bot\s+l√†\s+g√¨',
        
        # H·ªèi linh tinh
        r'k·ªÉ\s+chuy·ªán',
        r'h√°t\s+b√†i',
        r'l√†m\s+th∆°',
        r'ƒë·ªë\s+vui',
        r'ch∆°i\s+game'
    ]
    
    import re
    for pattern in irrelevant_patterns:
        if re.search(pattern, query_lower):
            return True
    
    return False

def get_irrelevant_response(query):
    """
    Tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi cho c√°c c√¢u h·ªèi kh√¥ng li√™n quan
    Input: query (str)
    Output: response (str)
    """
    query_lower = query.lower().strip()
    
    # Ch√†o h·ªèi
    if any(word in query_lower for word in ['ch√†o', 'hello', 'hi', 'xin ch√†o']):
        return "üëã Xin ch√†o! T√¥i l√† ChatBot h·ªó tr·ª£ n·ªôi b·ªô c·ªßa c√¥ng ty. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ quy ƒë·ªãnh, ch√≠nh s√°ch c√¥ng ty. B·∫°n c√≥ c√¢u h·ªèi g√¨ v·ªÅ c√¥ng vi·ªác kh√¥ng?"
    
    # H·ªèi s·ª©c kh·ªèe
    elif any(word in query_lower for word in ['kh·ªèe', 'sao', 'th·∫ø n√†o']):
        return "üòä C·∫£m ∆°n b·∫°n ƒë√£ quan t√¢m! T√¥i l√† AI n√™n kh√¥ng c√≥ c·∫£m x√∫c nh∆∞ con ng∆∞·ªùi. T√¥i lu√¥n s·∫µn s√†ng h·ªó tr·ª£ b·∫°n v·ªõi c√°c c√¢u h·ªèi v·ªÅ quy ƒë·ªãnh c√¥ng ty. B·∫°n c·∫ßn t√¨m hi·ªÉu g√¨ v·ªÅ ch√≠nh s√°ch c√¥ng ty kh√¥ng?"
    
    # H·ªèi s·ªü th√≠ch
    elif any(word in query_lower for word in ['th√≠ch', 's·ªü th√≠ch']):
        return "ü§ñ T√¥i l√† AI n√™n kh√¥ng c√≥ s·ªü th√≠ch c√° nh√¢n. S·ªü th√≠ch c·ªßa t√¥i l√† gi√∫p ƒë·ª° nh√¢n vi√™n t√¨m hi·ªÉu v·ªÅ quy ƒë·ªãnh v√† ch√≠nh s√°ch c√¥ng ty. B·∫°n c√≥ c√¢u h·ªèi g√¨ v·ªÅ c√¥ng vi·ªác kh√¥ng?"
    
    # H·ªèi v·ªÅ AI/bot
    elif any(word in query_lower for word in ['l√† ai', 'l√† g√¨', 'ai', 'bot']):
        return "ü§ñ T√¥i l√† ChatBot h·ªó tr·ª£ n·ªôi b·ªô, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ quy ƒë·ªãnh, ch√≠nh s√°ch c√¥ng ty. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m hi·ªÉu v·ªÅ gi·ªù l√†m vi·ªác, ƒë·ªìng ph·ª•c, ƒë√≥ng g√≥i h√†ng h√≥a, v.v. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨?"
    
    # C√¢u h·ªèi linh tinh kh√°c
    else:
        return "üòÖ T√¥i ch·ªâ c√≥ th·ªÉ h·ªó tr·ª£ c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn quy ƒë·ªãnh v√† ch√≠nh s√°ch c√¥ng ty. B·∫°n c√≥ th·ªÉ h·ªèi t√¥i v·ªÅ:\n‚Ä¢ Gi·ªù l√†m vi·ªác\n‚Ä¢ Ch√≠nh s√°ch ƒë·ªìng ph·ª•c\n‚Ä¢ Quy ƒë·ªãnh ƒë√≥ng g√≥i h√†ng h√≥a\n‚Ä¢ Ch√≠nh s√°ch ngh·ªâ ph√©p\n‚Ä¢ V√† nhi·ªÅu ch·ªß ƒë·ªÅ kh√°c..."

def detect_typos_and_suggest(query):
    """
    Ph√°t hi·ªán l·ªói ch√≠nh t·∫£ v√† ƒë∆∞a ra g·ª£i √Ω
    Input: query (str)
    Output: suggestion (str) n·∫øu c√≥ l·ªói ch√≠nh t·∫£, None n·∫øu kh√¥ng
    """
    query_lower = query.lower().strip()
    
    # T·ª´ kh√≥a ch√≠nh x√°c v√† c√°c bi·∫øn th·ªÉ c√≥ th·ªÉ sai ch√≠nh t·∫£
    corrections = {
        # Gi·ªù l√†m vi·ªác
        'gi·ªù l√†m vi·ªác': ['gi·ªù l√†m vi·ªác', 'th·ªùi gian l√†m vi·ªác', 'gi·ªù ƒëi l√†m'],
        'ƒë·ªìng ph·ª•c': ['ƒë·ªìng ph·ª•c', 'qu·∫ßn √°o c√¥ng ty', 'trang ph·ª•c'],
        'ƒë√≥ng g√≥i': ['ƒë√≥ng g√≥i', 'bao b√¨', 'ƒë√≥ng g√≥i h√†ng h√≥a'],
        'ngh·ªâ ph√©p': ['ngh·ªâ ph√©p', 'ngh·ªâ vi·ªác', 'ngh·ªâ c√≥ l∆∞∆°ng'],
        'l∆∞∆°ng': ['l∆∞∆°ng', 'ti·ªÅn l∆∞∆°ng', 'th√π lao'],
        'b·∫£o hi·ªÉm': ['b·∫£o hi·ªÉm', 'b·∫£o hi·ªÉm x√£ h·ªôi', 'b·∫£o hi·ªÉm y t·∫ø'],
        'h√†ng h√≥a': ['h√†ng h√≥a', 's·∫£n ph·∫©m', 'h√†ng g·ª≠i'],
        'v·∫≠n chuy·ªÉn': ['v·∫≠n chuy·ªÉn', 'g·ª≠i h√†ng', 'chuy·ªÉn ph√°t'],
        'internet': ['internet', 'm·∫°ng', 'wifi'],
        'ƒë√†o t·∫°o': ['ƒë√†o t·∫°o', 'h·ªçc t·∫≠p', 'training'],
        'kh√°ch h√†ng': ['kh√°ch h√†ng', 'client', 'customer']
    }
    
    # T√¨m t·ª´ kh√≥a g·∫ßn ƒë√∫ng
    for correct_word, variants in corrections.items():
        for variant in variants:
            if variant in query_lower and correct_word not in query_lower:
                return f"üí° C√≥ th·ªÉ b·∫°n mu·ªën h·ªèi v·ªÅ **{correct_word}**? T√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m hi·ªÉu v·ªÅ ch·ªß ƒë·ªÅ n√†y."
    
    return None

def handle_no_match():
    """Tr·∫£ v·ªÅ th√¥ng b√°o khi kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£"""
    answer = "‚ùå Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n.\n\nüí° B·∫°n c√≥ th·ªÉ th·ª≠:\n‚Ä¢ Di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi m·ªôt c√°ch r√µ r√†ng h∆°n\n‚Ä¢ S·ª≠ d·ª•ng t·ª´ kh√≥a ch√≠nh x√°c\n‚Ä¢ H·ªèi v·ªÅ c√°c ch·ªß ƒë·ªÅ nh∆∞: gi·ªù l√†m vi·ªác, ƒë·ªìng ph·ª•c, ƒë√≥ng g√≥i h√†ng h√≥a, ngh·ªâ ph√©p, l∆∞∆°ng, b·∫£o hi·ªÉm\n\nü§ñ T√¥i lu√¥n s·∫µn s√†ng h·ªó tr·ª£ b·∫°n!"
    logging.info(f"No-match answer: {answer}")
    return answer

def check_topic_query(query, json_files):
    """
    Ki·ªÉm tra xem query c√≥ ph·∫£i l√† h·ªèi v·ªÅ ch·ªß ƒë·ªÅ (title) kh√¥ng
    Input: query (str), json_files (list of file paths)
    Output: answer (str) n·∫øu t√¨m th·∫•y ch·ªß ƒë·ªÅ, None n·∫øu kh√¥ng
    """
    query_lower = query.lower().strip()
    logging.info(f"Checking topic query: {query}")
    
    # C√°c t·ª´ kh√≥a ƒë·ªÉ nh·∫≠n di·ªán c√¢u h·ªèi v·ªÅ ch·ªß ƒë·ªÅ
    topic_keywords = [
        'g·ªìm nh·ªØng g√¨', 'c√≥ g√¨', 'bao g·ªìm', 'li·ªát k√™', 'k·ªÉ t√™n', 'n√™u ra',
        'c√°c lo·∫°i', 'danh s√°ch', 'chi ti·∫øt', 'th√¥ng tin', 'quy ƒë·ªãnh',
         'h∆∞·ªõng d·∫´n', 'c√°ch th·ª©c', 'quy tr√¨nh', 't·∫•t c·∫£ v·ªÅ', 'm·ªçi th·ª©'
         'm·ªçi th·ª© v·ªÅ'
    ]
    
    # Ki·ªÉm tra xem c√≥ ph·∫£i c√¢u h·ªèi v·ªÅ ch·ªß ƒë·ªÅ kh√¥ng
    is_topic_query = any(keyword in query_lower for keyword in topic_keywords)
    
    # N·∫øu kh√¥ng c√≥ t·ª´ kh√≥a ch·ªß ƒë·ªÅ, ki·ªÉm tra xem c√≥ ph·∫£i c√¢u h·ªèi t·ªïng qu√°t kh√¥ng
    if not is_topic_query:
        general_questions = [
            'c√≥ nh·ªØng g√¨', 'g·ªìm g√¨', 'bao g·ªìm g√¨', 'c√≥ c√°i g√¨',
            'th√¥ng tin g√¨', 'quy ƒë·ªãnh g√¨', 'ch√≠nh s√°ch g√¨','n√™u','n√™u ra'
            ,'t·∫•t t·∫ßn t·∫≠t ','m·ªçi th·ª©'
        ]
        is_topic_query = any(q in query_lower for q in general_questions)
    
    if not is_topic_query:
        return None
    
    # T√¨m ki·∫øm ch·ªß ƒë·ªÅ ph√π h·ª£p
    best_match = None
    best_similarity = 0
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for item in data['data']:
                    title = item['title'].lower()
                    
                    # Ki·ªÉm tra similarity gi·ªØa query v√† title
                    similarity = SequenceMatcher(None, query_lower, title).ratio()
                    
                    # Ki·ªÉm tra t·ª´ kh√≥a chung
                    keyword_match = 0
                    query_words = query_lower.split()
                    title_words = title.split()
                    
                    for q_word in query_words:
                        for t_word in title_words:
                            if q_word in t_word or t_word in q_word:
                                keyword_match += 1
                                break
                    
                    # T√≠nh combined score
                    combined_score = similarity + 0.1 * keyword_match
                    
                    # N·∫øu similarity cao ho·∫∑c c√≥ t·ª´ kh√≥a ch√≠nh trong title
                    if combined_score > best_similarity and (combined_score > 0.5 or keyword_match > 0):
                        best_similarity = combined_score
                        best_match = {
                            'title': item['title'],
                            'contexts': [paragraph['context'] for paragraph in item['paragraphs']],
                            'file': json_file,
                            'score': combined_score
                        }
        except Exception as e:
            logging.error(f"Error reading {json_file}: {e}")
    
    if best_match and best_similarity > 0.4:
        # T·∫°o c√¢u tr·∫£ l·ªùi t·ªïng h·ª£p
        answer = f"üìã **{best_match['title']}**\n\n"
        for i, context in enumerate(best_match['contexts'], 1):
            answer += f"**{i}.** {context}\n\n"
        
        logging.info(f"Topic match found: {best_match['title']} (score: {best_similarity:.3f})")
        return answer
    
    return None

def qa_pipeline(query, json_files):
    """
    Pipeline x·ª≠ l√Ω query v√† tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi
    Input: query (str), json_files (list of file paths)
    Output: answer (str)
    """
    # B∆∞·ªõc 1: Ki·ªÉm tra c√¢u h·ªèi kh√¥ng li√™n quan
    if detect_irrelevant_questions(query):
        logging.info(f"Irrelevant question detected: {query}")
        return get_irrelevant_response(query)
    
    # B∆∞·ªõc 2: Ki·ªÉm tra l·ªói ch√≠nh t·∫£ v√† ƒë∆∞a ra g·ª£i √Ω
    typo_suggestion = detect_typos_and_suggest(query)
    if typo_suggestion:
        logging.info(f"Typo suggestion provided for: {query}")
        return typo_suggestion
    
    # B∆∞·ªõc 3: Ki·ªÉm tra c√¢u h·ªèi v·ªÅ ch·ªß ƒë·ªÅ
    topic_answer = check_topic_query(query, json_files)
    if topic_answer:
        return topic_answer
    
    # B∆∞·ªõc 4: Exact question check
    answer = exact_question_check(query, json_files)
    if answer:
        return answer
    
    # B∆∞·ªõc 5: Semantic search v·ªõi fallback
    data, vectorizer, doc_vectors = load_data(json_files)
    if not data or vectorizer is None:
        return handle_no_match()
    
    # Preprocessing
    processed_query = preprocess_query(query)
    
    # NER extraction
    entities = ner_extraction(query)
    
    # Semantic search
    top_results = semantic_search(query, data, vectorizer, doc_vectors, entities)
    
    # Rule-based matching
    best_idx = rule_based_matching(query, data, top_results, entities)
    
    # Reranking / Fusion
    final_idx = rerank_fusion(top_results, best_idx, data)
    
    # Postprocessing v·ªõi fallback th√¥ng minh
    if final_idx is None:
        # Ki·ªÉm tra xem c√≥ ph·∫£i c√¢u h·ªèi qu√° ng·∫Øn ho·∫∑c kh√¥ng r√µ r√†ng kh√¥ng
        if len(processed_query.split()) < 2:
            return "ü§î C√¢u h·ªèi c·ªßa b·∫°n h∆°i ng·∫Øn. B·∫°n c√≥ th·ªÉ m√¥ t·∫£ chi ti·∫øt h∆°n v·ªÅ v·∫•n ƒë·ªÅ c·∫ßn h·ªó tr·ª£ kh√¥ng?\n\nüí° V√≠ d·ª•: 'Gi·ªù l√†m vi·ªác c·ªßa c√¥ng ty nh∆∞ th·∫ø n√†o?' thay v√¨ ch·ªâ 'gi·ªù l√†m vi·ªác'"
        
        # Ki·ªÉm tra xem c√≥ ph·∫£i c√¢u h·ªèi v·ªÅ ch·ªß ƒë·ªÅ kh√¥ng r√µ r√†ng kh√¥ng
        if any(word in processed_query for word in ['g√¨', 'n√†o', 'sao', 'th·∫ø n√†o']):
            return "ü§î B·∫°n c√≥ th·ªÉ c·ª• th·ªÉ h∆°n v·ªÅ ch·ªß ƒë·ªÅ c·∫ßn t√¨m hi·ªÉu kh√¥ng?\n\nüí° T√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªÅ:\n‚Ä¢ Gi·ªù l√†m vi·ªác\n‚Ä¢ Ch√≠nh s√°ch ƒë·ªìng ph·ª•c\n‚Ä¢ Quy ƒë·ªãnh ƒë√≥ng g√≥i h√†ng h√≥a\n‚Ä¢ Ch√≠nh s√°ch ngh·ªâ ph√©p\n‚Ä¢ V√† nhi·ªÅu ch·ªß ƒë·ªÅ kh√°c..."
        
        return handle_no_match()
    
    answer = data[final_idx]['answer']
    return postprocess_answer(answer, entities)

# Flask Web App
from flask import Flask, render_template, request, jsonify
import os

app = Flask(__name__)

# Initialize data
json_files = [
    r"D:\du_an_chatBot_noi_bo\data\duLieuCongTy.json",
    r"D:\du_an_chatBot_noi_bo\data\data_chuDe_1.json",
    r"D:\du_an_chatBot_noi_bo\data\data_chuDe_2.json",
    r"D:\du_an_chatBot_noi_bo\data\data_chuDe_3.json",
    r"D:\du_an_chatBot_noi_bo\data\data_chuDe_4.json",
    r"D:\du_an_chatBot_noi_bo\data\data_chuDe_5.json",
    r"D:\du_an_chatBot_noi_bo\data\data_chuDe_7.json",
    r"D:\du_an_chatBot_noi_bo\data\data_chuDe_8.json",
    r"D:\du_an_chatBot_noi_bo\data\data_chuDe_9.json"
]

@app.route('/')
def index():
    """Trang ch·ªß"""
    return render_template('index.html')

@app.route('/api/ask', methods=['POST'])
def ask_question():
    """API endpoint ƒë·ªÉ h·ªèi c√¢u h·ªèi"""
    try:
        data = request.get_json()
        question = data.get('question', '').strip()
        
        if not question:
            return jsonify({'error': 'Vui l√≤ng nh·∫≠p c√¢u h·ªèi'}), 400
        
        # S·ª≠ d·ª•ng logic c√≥ s·∫µn trong qa_pipeline
        answer = qa_pipeline(question, json_files)
        
        return jsonify({
            'question': question,
            'answer': answer,
            'status': 'success'
        })
    
    except Exception as e:
        logging.error(f"Error processing question: {e}")
        return jsonify({'error': 'C√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi'}), 500

@app.route('/api/health')
def health_check():
    """Health check endpoint"""
    return jsonify({'status': 'healthy', 'message': 'API ƒëang ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng'})

# Example usage (ch·ªâ ch·∫°y khi kh√¥ng ph·∫£i Flask app)
if __name__ == "__main__":
    # import sys
    
    # # Ki·ªÉm tra n·∫øu c√≥ argument 'web' th√¨ ch·∫°y Flask app
    # if len(sys.argv) > 1 and sys.argv[1] == 'web':
        print("üöÄ Starting Flask web app...")
        print("üì± Open your browser and go to: http://localhost:5000")
        app.run(debug=True, host='0.0.0.0', port=5000)
    # else:
    #     # Ch·∫°y test nh∆∞ c≈©
    #     query = "C√°ch gi·ªØ ƒë·ªìng ph·ª•c lu√¥n s·∫°ch s·∫Ω   "
    #     answer = qa_pipeline(query, json_files)
        
    #     # Safe printing with encoding handling
    #     os.system('chcp 65001 >nul 2>&1')  # Set console to UTF-8
        
    #     try:
    #         print(f"Query: {query}")
    #         print(f"Answer: {answer}")
    #     except:
    #         # Fallback: write to file
    #         with open('result.txt', 'w', encoding='utf-8') as f:
    #             f.write(f"Query: {query}\n")
    #             f.write(f"Answer: {answer}\n")
    #         print("Results saved to result.txt")